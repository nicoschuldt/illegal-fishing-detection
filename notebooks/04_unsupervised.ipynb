{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trip features data...\n",
      "Loaded 3591 trips with 52 features\n",
      "Fishing trips: 2495 (69.5%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "import warnings\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the trip features data generated in notebook 02\n",
    "print(\"Loading trip features data...\")\n",
    "trip_features_df = pd.read_csv('engineered_trip_features.csv')\n",
    "\n",
    "print(f\"Loaded {len(trip_features_df)} trips with {len(trip_features_df.columns)} features\")\n",
    "print(f\"Fishing trips: {trip_features_df['is_fishing'].sum()} ({trip_features_df['is_fishing'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 1: Validating cluster quality...\n",
      "Using 47 features for clustering analysis\n",
      "\n",
      "Optimal number of clusters:\n",
      "  Silhouette score: k = 2 (score: 0.213)\n",
      "  Calinski-Harabasz: k = 2 (score: 753.1)\n"
     ]
    }
   ],
   "source": [
    "# ##############################################################################\n",
    "# Section 1: Clustering Validation and Stability Analysis\n",
    "# ##############################################################################\n",
    "\n",
    "\"\"\"\n",
    "First, we'll validate the K-means clusters found in notebook 02 and assess\n",
    "their stability and meaningfulness using established metrics.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nSection 1: Validating cluster quality...\")\n",
    "\n",
    "# Prepare the same feature set used in notebook 02\n",
    "cluster_features = trip_features_df.drop(['trip_id', 'mmsi', 'start_time', 'end_time', 'is_fishing'], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "for col in cluster_features.columns:\n",
    "    if cluster_features[col].isna().any():\n",
    "        cluster_features[col] = cluster_features[col].fillna(cluster_features[col].median())\n",
    "\n",
    "# Standardize features for clustering\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(cluster_features)\n",
    "\n",
    "print(f\"Using {scaled_features.shape[1]} features for clustering analysis\")\n",
    "\n",
    "# Evaluate different numbers of clusters using multiple metrics\n",
    "k_range = range(2, 11)\n",
    "metrics = {\n",
    "    'inertia': [],\n",
    "    'silhouette': [],\n",
    "    'calinski_harabasz': []\n",
    "}\n",
    "\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "for k in k_range:\n",
    "    # Fit K-means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(scaled_features)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics['inertia'].append(kmeans.inertia_)\n",
    "    metrics['silhouette'].append(silhouette_score(scaled_features, cluster_labels))\n",
    "    metrics['calinski_harabasz'].append(calinski_harabasz_score(scaled_features, cluster_labels))\n",
    "\n",
    "# Plot clustering validation metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Elbow curve (inertia)\n",
    "axes[0].plot(k_range, metrics['inertia'], 'o-')\n",
    "axes[0].set_title('Elbow Method (Inertia)')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Silhouette score\n",
    "axes[1].plot(k_range, metrics['silhouette'], 'o-', color='orange')\n",
    "axes[1].set_title('Silhouette Score')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Calinski-Harabasz score\n",
    "axes[2].plot(k_range, metrics['calinski_harabasz'], 'o-', color='green')\n",
    "axes[2].set_title('Calinski-Harabasz Score')\n",
    "axes[2].set_xlabel('Number of Clusters (k)')\n",
    "axes[2].set_ylabel('CH Score')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_validation_metrics.png')\n",
    "plt.close()\n",
    "\n",
    "# Determine optimal k based on metrics\n",
    "optimal_k_silhouette = k_range[np.argmax(metrics['silhouette'])]\n",
    "optimal_k_ch = k_range[np.argmax(metrics['calinski_harabasz'])]\n",
    "\n",
    "print(f\"\\nOptimal number of clusters:\")\n",
    "print(f\"  Silhouette score: k = {optimal_k_silhouette} (score: {max(metrics['silhouette']):.3f})\")\n",
    "print(f\"  Calinski-Harabasz: k = {optimal_k_ch} (score: {max(metrics['calinski_harabasz']):.1f})\")\n",
    "\n",
    "# Use the optimal k for detailed analysis\n",
    "optimal_k = optimal_k_silhouette  # Choose based on silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 2: Detailed analysis with k = 2...\n",
      "\n",
      "Cluster Analysis Summary:\n",
      "================================================================================\n",
      "\n",
      "Cluster 0 (2491.0 trips, 69.4%):\n",
      "  Fishing rate: 67.2%\n",
      "  Avg duration: 12.9 hours\n",
      "  Avg speed: 5.6 knots\n",
      "  Path efficiency: 0.686\n",
      "  Course changes/hour: 0.81\n",
      "\n",
      "Cluster 1 (1100.0 trips, 30.6%):\n",
      "  Fishing rate: 74.6%\n",
      "  Avg duration: 11.2 hours\n",
      "  Avg speed: 6.6 knots\n",
      "  Path efficiency: 0.767\n",
      "  Course changes/hour: 0.89\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ##############################################################################\n",
    "# Section 2: Detailed Cluster Analysis\n",
    "# ##############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Analyze the optimal clusters in detail to understand what behavioral patterns\n",
    "they represent and how they relate to fishing activity.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nSection 2: Detailed analysis with k = {optimal_k}...\")\n",
    "\n",
    "# Fit final clustering model\n",
    "final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "trip_features_df['cluster'] = final_kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Calculate comprehensive cluster statistics\n",
    "cluster_analysis = []\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = trip_features_df[trip_features_df['cluster'] == cluster_id]\n",
    "    \n",
    "    analysis = {\n",
    "        'cluster': cluster_id,\n",
    "        'count': len(cluster_data),\n",
    "        'percentage': len(cluster_data) / len(trip_features_df) * 100,\n",
    "        \n",
    "        # Fishing behavior\n",
    "        'fishing_proportion_mean': cluster_data['fishing_proportion'].mean(),\n",
    "        'fishing_proportion_std': cluster_data['fishing_proportion'].std(),\n",
    "        'is_fishing_rate': cluster_data['is_fishing'].mean(),\n",
    "        \n",
    "        # Trip characteristics\n",
    "        'duration_hours_mean': cluster_data['duration_hours'].mean(),\n",
    "        'duration_hours_std': cluster_data['duration_hours'].std(),\n",
    "        'point_count_mean': cluster_data['point_count'].mean(),\n",
    "        \n",
    "        # Movement patterns\n",
    "        'speed_mean_avg': cluster_data['speed_mean'].mean(),\n",
    "        'speed_std_avg': cluster_data['speed_std'].mean(),\n",
    "        'path_efficiency_avg': cluster_data['path_efficiency'].mean(),\n",
    "        'course_changes_per_hour_avg': cluster_data['course_changes_per_hour'].mean(),\n",
    "        \n",
    "        # Spatial patterns\n",
    "        'distance_from_shore_mean_avg': cluster_data['distance_from_shore_mean'].mean(),\n",
    "        'distance_from_port_mean_avg': cluster_data['distance_from_port_mean'].mean(),\n",
    "    }\n",
    "    \n",
    "    cluster_analysis.append(analysis)\n",
    "\n",
    "cluster_analysis_df = pd.DataFrame(cluster_analysis)\n",
    "\n",
    "print(\"\\nCluster Analysis Summary:\")\n",
    "print(\"=\" * 80)\n",
    "for _, cluster in cluster_analysis_df.iterrows():\n",
    "    print(f\"\\nCluster {int(cluster['cluster'])} ({cluster['count']} trips, {cluster['percentage']:.1f}%):\")\n",
    "    print(f\"  Fishing rate: {cluster['is_fishing_rate']:.1%}\")\n",
    "    print(f\"  Avg duration: {cluster['duration_hours_mean']:.1f} hours\")\n",
    "    print(f\"  Avg speed: {cluster['speed_mean_avg']:.1f} knots\")\n",
    "    print(f\"  Path efficiency: {cluster['path_efficiency_avg']:.3f}\")\n",
    "    print(f\"  Course changes/hour: {cluster['course_changes_per_hour_avg']:.2f}\")\n",
    "\n",
    "# Visualize cluster characteristics\n",
    "key_metrics = [\n",
    "    'fishing_proportion_mean', 'duration_hours_mean', 'speed_mean_avg', \n",
    "    'path_efficiency_avg', 'course_changes_per_hour_avg'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(key_metrics), 1, figsize=(12, 4*len(key_metrics)))\n",
    "\n",
    "for i, metric in enumerate(key_metrics):\n",
    "    sns.barplot(x='cluster', y=metric, data=cluster_analysis_df, ax=axes[i])\n",
    "    axes[i].set_title(f'{metric.replace(\"_\", \" \").title()} by Cluster')\n",
    "    axes[i].set_xlabel('Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_characteristics_detailed.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 3: Comparing clustering methods...\n",
      "Applying DBSCAN clustering...\n",
      "\n",
      "DBSCAN Results:\n",
      "   eps  n_clusters  n_noise  silhouette\n",
      "0  0.3           0     3591          -1\n",
      "1  0.5           0     3591          -1\n",
      "2  0.7           0     3591          -1\n",
      "3  1.0           0     3591          -1\n",
      "\n",
      "Best DBSCAN: eps=0.3, silhouette=-1.000\n",
      "\n",
      "Applying Hierarchical clustering...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ##############################################################################\n",
    "# Section 3: Alternative Clustering Methods\n",
    "# ##############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Compare K-means results with other clustering algorithms to validate\n",
    "our findings and potentially discover different patterns.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nSection 3: Comparing clustering methods...\")\n",
    "\n",
    "# 1. DBSCAN Clustering\n",
    "print(\"Applying DBSCAN clustering...\")\n",
    "\n",
    "# Try different epsilon values for DBSCAN\n",
    "eps_values = [0.3, 0.5, 0.7, 1.0]\n",
    "dbscan_results = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=10)\n",
    "    dbscan_labels = dbscan.fit_predict(scaled_features)\n",
    "    \n",
    "    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    n_noise = list(dbscan_labels).count(-1)\n",
    "    \n",
    "    if n_clusters > 1:\n",
    "        # Calculate silhouette score (excluding noise points)\n",
    "        mask = dbscan_labels != -1\n",
    "        if mask.sum() > 10:  # Need enough points for silhouette score\n",
    "            sil_score = silhouette_score(scaled_features[mask], dbscan_labels[mask])\n",
    "        else:\n",
    "            sil_score = -1\n",
    "    else:\n",
    "        sil_score = -1\n",
    "    \n",
    "    dbscan_results.append({\n",
    "        'eps': eps,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'silhouette': sil_score\n",
    "    })\n",
    "\n",
    "dbscan_df = pd.DataFrame(dbscan_results)\n",
    "print(\"\\nDBSCAN Results:\")\n",
    "print(dbscan_df)\n",
    "\n",
    "# Select best DBSCAN result\n",
    "best_dbscan = dbscan_df[dbscan_df['silhouette'] == dbscan_df['silhouette'].max()]\n",
    "if not best_dbscan.empty:\n",
    "    best_eps = best_dbscan.iloc[0]['eps']\n",
    "    print(f\"\\nBest DBSCAN: eps={best_eps}, silhouette={best_dbscan.iloc[0]['silhouette']:.3f}\")\n",
    "    \n",
    "    # Apply best DBSCAN\n",
    "    dbscan_final = DBSCAN(eps=best_eps, min_samples=10)\n",
    "    dbscan_labels = dbscan_final.fit_predict(scaled_features)\n",
    "    trip_features_df['dbscan_cluster'] = dbscan_labels\n",
    "\n",
    "# 2. Hierarchical Clustering\n",
    "print(\"\\nApplying Hierarchical clustering...\")\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(scaled_features[:1000], method='ward')  # Sample for efficiency\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=10)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.savefig('hierarchical_dendrogram.png')\n",
    "plt.close()\n",
    "\n",
    "# Cut dendrogram to get clusters\n",
    "hierarchical_labels = fcluster(linkage_matrix, optimal_k, criterion='maxclust')\n",
    "\n",
    "# Extend to full dataset (simplified approach)\n",
    "# In practice, you'd apply hierarchical clustering to full dataset\n",
    "trip_features_df['hierarchical_cluster'] = np.nan\n",
    "trip_features_df.iloc[:1000, trip_features_df.columns.get_loc('hierarchical_cluster')] = hierarchical_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 4: Business interpretation of clusters...\n",
      "\n",
      "Cluster Interpretations:\n",
      "==================================================\n",
      "Cluster 0: Mixed Operations\n",
      "Cluster 1: Mixed Operations\n",
      "\n",
      "Statistical validation of cluster differences:\n",
      "  speed_mean: F=111.00, p=0.000000 ***\n",
      "  fishing_proportion: F=21.96, p=0.000003 ***\n",
      "  path_efficiency: F=44.42, p=0.000000 ***\n",
      "  duration_hours: F=5.21, p=0.022532 *\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ##############################################################################\n",
    "# Section 4: Cluster Interpretation and Business Insights\n",
    "# ##############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Interpret the clusters in terms of fishing operations and validate that\n",
    "they correspond to meaningful behavioral patterns.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nSection 4: Business interpretation of clusters...\")\n",
    "\n",
    "# Define cluster interpretations based on characteristics\n",
    "def interpret_cluster(cluster_data):\n",
    "    \"\"\"Interpret cluster based on its characteristics.\"\"\"\n",
    "    fishing_rate = cluster_data['is_fishing_rate']\n",
    "    avg_speed = cluster_data['speed_mean_avg']\n",
    "    duration = cluster_data['duration_hours_mean']\n",
    "    efficiency = cluster_data['path_efficiency_avg']\n",
    "    \n",
    "    if fishing_rate > 0.8 and avg_speed < 5 and efficiency < 0.7:\n",
    "        return \"Active Fishing Operations\"\n",
    "    elif fishing_rate > 0.7 and avg_speed < 6 and efficiency < 0.8:\n",
    "        return \"Mixed Fishing/Transit\"\n",
    "    elif fishing_rate < 0.4 and avg_speed > 7 and efficiency > 0.9:\n",
    "        return \"Pure Transit/Travel\"\n",
    "    elif fishing_rate > 0.6 and avg_speed < 4:\n",
    "        return \"Intensive Fishing (Slow)\"\n",
    "    else:\n",
    "        return \"Mixed Operations\"\n",
    "\n",
    "# Apply interpretations\n",
    "cluster_analysis_df['interpretation'] = cluster_analysis_df.apply(interpret_cluster, axis=1)\n",
    "\n",
    "print(\"\\nCluster Interpretations:\")\n",
    "print(\"=\" * 50)\n",
    "for _, cluster in cluster_analysis_df.iterrows():\n",
    "    print(f\"Cluster {int(cluster['cluster'])}: {cluster['interpretation']}\")\n",
    "\n",
    "# Validate interpretations with statistical tests\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "print(\"\\nStatistical validation of cluster differences:\")\n",
    "\n",
    "# Test if clusters are significantly different in key metrics\n",
    "metrics_to_test = ['speed_mean', 'fishing_proportion', 'path_efficiency', 'duration_hours']\n",
    "\n",
    "for metric in metrics_to_test:\n",
    "    cluster_groups = [trip_features_df[trip_features_df['cluster'] == i][metric].values \n",
    "                     for i in range(optimal_k)]\n",
    "    \n",
    "    # Remove empty groups\n",
    "    cluster_groups = [group for group in cluster_groups if len(group) > 0]\n",
    "    \n",
    "    if len(cluster_groups) > 1:\n",
    "        f_stat, p_value = f_oneway(*cluster_groups)\n",
    "        print(f\"  {metric}: F={f_stat:.2f}, p={p_value:.6f} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 5: Using clusters to improve supervised learning...\n",
      "Baseline model accuracy: 0.8663\n",
      "Enhanced model accuracy: 0.8552\n",
      "Improvement: -0.0111\n",
      "Cluster information does not significantly improve supervised learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ##############################################################################\n",
    "# Section 5: Clustering Insights for Supervised Learning\n",
    "# ##############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Investigate whether cluster information can improve our supervised\n",
    "fishing detection model.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nSection 5: Using clusters to improve supervised learning...\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepare data for supervised learning with cluster features\n",
    "vessels = trip_features_df['mmsi'].unique()\n",
    "train_vessels, test_vessels = train_test_split(vessels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Features for modeling\n",
    "model_features = [col for col in trip_features_df.columns \n",
    "                 if col not in ['trip_id', 'mmsi', 'start_time', 'end_time', 'is_fishing', 'fishing_proportion']]\n",
    "\n",
    "# Test 1: Original features only (baseline)\n",
    "baseline_features = [f for f in model_features if 'cluster' not in f]\n",
    "X_baseline = trip_features_df[baseline_features]\n",
    "y = trip_features_df['is_fishing']\n",
    "\n",
    "# Handle missing values\n",
    "for col in X_baseline.columns:\n",
    "    if X_baseline[col].isna().any():\n",
    "        X_baseline[col] = X_baseline[col].fillna(X_baseline[col].median())\n",
    "\n",
    "X_train_baseline = X_baseline[trip_features_df['mmsi'].isin(train_vessels)]\n",
    "X_test_baseline = X_baseline[trip_features_df['mmsi'].isin(test_vessels)]\n",
    "y_train = y[trip_features_df['mmsi'].isin(train_vessels)]\n",
    "y_test = y[trip_features_df['mmsi'].isin(test_vessels)]\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "baseline_model.fit(X_train_baseline, y_train)\n",
    "baseline_score = baseline_model.score(X_test_baseline, y_test)\n",
    "\n",
    "print(f\"Baseline model accuracy: {baseline_score:.4f}\")\n",
    "\n",
    "# Test 2: Features + cluster information\n",
    "cluster_features_list = ['cluster']  # Add other cluster variants if available\n",
    "if 'dbscan_cluster' in trip_features_df.columns:\n",
    "    cluster_features_list.append('dbscan_cluster')\n",
    "\n",
    "enhanced_features = baseline_features + cluster_features_list\n",
    "X_enhanced = trip_features_df[enhanced_features]\n",
    "\n",
    "for cluster_col in cluster_features_list:\n",
    "    if cluster_col in X_enhanced.columns:\n",
    "        cluster_dummies = pd.get_dummies(X_enhanced[cluster_col], prefix=cluster_col)\n",
    "        X_enhanced = pd.concat([X_enhanced.drop(cluster_col, axis=1), cluster_dummies], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "for col in X_enhanced.columns:\n",
    "    if X_enhanced[col].isna().any():\n",
    "        X_enhanced[col] = X_enhanced[col].fillna(X_enhanced[col].median())\n",
    "\n",
    "X_train_enhanced = X_enhanced[trip_features_df['mmsi'].isin(train_vessels)]\n",
    "X_test_enhanced = X_enhanced[trip_features_df['mmsi'].isin(test_vessels)]\n",
    "\n",
    "# Train enhanced model\n",
    "enhanced_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "enhanced_model.fit(X_train_enhanced, y_train)\n",
    "enhanced_score = enhanced_model.score(X_test_enhanced, y_test)\n",
    "\n",
    "print(f\"Enhanced model accuracy: {enhanced_score:.4f}\")\n",
    "print(f\"Improvement: {enhanced_score - baseline_score:.4f}\")\n",
    "\n",
    "# Test statistical significance of improvement\n",
    "if enhanced_score > baseline_score:\n",
    "    print(\"Cluster information provides improvement to supervised learning\")\n",
    "else:\n",
    "    print(\"Cluster information does not significantly improve supervised learning\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
